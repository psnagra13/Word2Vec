{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.tsv', sep='\\t')\n",
    "data = train_data.title\n",
    "data = list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['how', 'to', 'draw', 'a', 'stacked', 'dotplot', 'in', 'r', '?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitSentence(sentence):\n",
    "    sentence = sentence.lower()\n",
    "#     res = gensim.utils.simple_preprocess ('How to draw a stacked dotplot in R?')\n",
    "    res = sentence.split(' ')\n",
    "    res = word_tokenize(sentence)\n",
    "    return res\n",
    "splitSentence('How to draw a stacked dotplot in R?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 18:07:36,023 : INFO : Done Processing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents = []\n",
    "for row in data:\n",
    "    documents.append(splitSentence(row))\n",
    "logging.info (\"Done Processing\")    \n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 18:07:36,029 : INFO : collecting all words and their counts\n",
      "2019-03-15 18:07:36,030 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-15 18:07:36,051 : INFO : PROGRESS: at sentence #10000, processed 93014 words, keeping 8922 word types\n",
      "2019-03-15 18:07:36,066 : INFO : PROGRESS: at sentence #20000, processed 185238 words, keeping 13435 word types\n",
      "2019-03-15 18:07:36,083 : INFO : PROGRESS: at sentence #30000, processed 278067 words, keeping 17059 word types\n",
      "2019-03-15 18:07:36,105 : INFO : PROGRESS: at sentence #40000, processed 370895 words, keeping 20252 word types\n",
      "2019-03-15 18:07:36,123 : INFO : PROGRESS: at sentence #50000, processed 464208 words, keeping 23233 word types\n",
      "2019-03-15 18:07:36,144 : INFO : PROGRESS: at sentence #60000, processed 556960 words, keeping 25993 word types\n",
      "2019-03-15 18:07:36,165 : INFO : PROGRESS: at sentence #70000, processed 649567 words, keeping 28521 word types\n",
      "2019-03-15 18:07:36,185 : INFO : PROGRESS: at sentence #80000, processed 742108 words, keeping 30991 word types\n",
      "2019-03-15 18:07:36,205 : INFO : PROGRESS: at sentence #90000, processed 834140 words, keeping 33415 word types\n",
      "2019-03-15 18:07:36,227 : INFO : collected 35731 word types from a corpus of 925908 raw words and 100000 sentences\n",
      "2019-03-15 18:07:36,227 : INFO : Loading a fresh vocabulary\n",
      "2019-03-15 18:07:36,252 : INFO : min_count=2 retains 13265 unique words (37% of original 35731, drops 22466)\n",
      "2019-03-15 18:07:36,253 : INFO : min_count=2 leaves 903442 word corpus (97% of original 925908, drops 22466)\n",
      "2019-03-15 18:07:36,292 : INFO : deleting the raw counts dictionary of 35731 items\n",
      "2019-03-15 18:07:36,293 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-03-15 18:07:36,294 : INFO : downsampling leaves estimated 688672 word corpus (76.2% of prior 903442)\n",
      "2019-03-15 18:07:36,329 : INFO : estimated required memory for 13265 words and 300 dimensions: 38468500 bytes\n",
      "2019-03-15 18:07:36,330 : INFO : resetting layer weights\n",
      "2019-03-15 18:07:36,473 : INFO : training model with 10 workers on 13265 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-15 18:07:36,983 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:36,984 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:36,991 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:36,994 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:37,009 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:37,019 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:37,020 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:37,027 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:37,034 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:37,042 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:37,043 : INFO : EPOCH - 1 : training on 925908 raw words (688655 effective words) took 0.6s, 1241156 effective words/s\n",
      "2019-03-15 18:07:37,565 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:37,591 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:37,594 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:37,597 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:37,599 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:37,600 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:37,600 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:37,603 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:37,613 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:37,616 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:37,616 : INFO : EPOCH - 2 : training on 925908 raw words (689078 effective words) took 0.6s, 1221536 effective words/s\n",
      "2019-03-15 18:07:38,137 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:38,145 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:38,146 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:38,147 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:38,152 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:38,157 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:38,159 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:38,161 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:38,169 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:38,172 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:38,172 : INFO : EPOCH - 3 : training on 925908 raw words (689186 effective words) took 0.5s, 1267022 effective words/s\n",
      "2019-03-15 18:07:38,689 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:38,706 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:38,711 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:38,721 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:38,726 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:38,730 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:38,731 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:38,732 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:38,734 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:38,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:38,736 : INFO : EPOCH - 4 : training on 925908 raw words (688531 effective words) took 0.6s, 1243361 effective words/s\n",
      "2019-03-15 18:07:39,241 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:39,253 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:39,260 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:39,276 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:39,278 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:39,288 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:39,300 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:39,304 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:39,318 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:39,320 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:39,322 : INFO : EPOCH - 5 : training on 925908 raw words (689054 effective words) took 0.6s, 1190634 effective words/s\n",
      "2019-03-15 18:07:39,325 : INFO : training on a 4629540 raw words (3444504 effective words) took 2.9s, 1208130 effective words/s\n",
      "2019-03-15 18:07:39,328 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-03-15 18:07:39,328 : INFO : training model with 10 workers on 13265 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-03-15 18:07:39,846 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:39,849 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:39,850 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:39,852 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:39,865 : INFO : worker thread finished; awaiting finish of 5 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 18:07:39,867 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:39,883 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:39,884 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:39,893 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:39,896 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:39,897 : INFO : EPOCH - 1 : training on 925908 raw words (688705 effective words) took 0.5s, 1257015 effective words/s\n",
      "2019-03-15 18:07:40,418 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:40,422 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:40,423 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:40,452 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:40,461 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:40,464 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:40,466 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:40,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:40,489 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:40,495 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:40,495 : INFO : EPOCH - 2 : training on 925908 raw words (688749 effective words) took 0.6s, 1168008 effective words/s\n",
      "2019-03-15 18:07:41,137 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:41,166 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:41,170 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:41,173 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:41,176 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:41,178 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:41,179 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:41,180 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:41,182 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:41,194 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:41,194 : INFO : EPOCH - 3 : training on 925908 raw words (688338 effective words) took 0.7s, 1003348 effective words/s\n",
      "2019-03-15 18:07:41,807 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:41,808 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:41,809 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:41,823 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:41,827 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:41,839 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:41,857 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:41,865 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:41,869 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:41,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:41,874 : INFO : EPOCH - 4 : training on 925908 raw words (688670 effective words) took 0.7s, 1025680 effective words/s\n",
      "2019-03-15 18:07:42,389 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:42,397 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:42,410 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:42,411 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:42,415 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:42,417 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:42,437 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:42,439 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:42,449 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:42,463 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:42,463 : INFO : EPOCH - 5 : training on 925908 raw words (688721 effective words) took 0.6s, 1185324 effective words/s\n",
      "2019-03-15 18:07:42,973 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:42,982 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:42,995 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:42,997 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:42,998 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:42,999 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:43,000 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:43,018 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:43,019 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:43,024 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:43,025 : INFO : EPOCH - 6 : training on 925908 raw words (688627 effective words) took 0.6s, 1248006 effective words/s\n",
      "2019-03-15 18:07:43,531 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:43,536 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:43,546 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:43,549 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:43,556 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:43,562 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:43,569 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:43,577 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:43,581 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:43,581 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:43,582 : INFO : EPOCH - 7 : training on 925908 raw words (688730 effective words) took 0.5s, 1254603 effective words/s\n",
      "2019-03-15 18:07:44,092 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:44,121 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:44,129 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:44,130 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:44,132 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:44,133 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:44,133 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:44,134 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:44,135 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:44,135 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:44,135 : INFO : EPOCH - 8 : training on 925908 raw words (688371 effective words) took 0.5s, 1270696 effective words/s\n",
      "2019-03-15 18:07:44,717 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:44,735 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:44,738 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:44,739 : INFO : worker thread finished; awaiting finish of 6 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 18:07:44,758 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:44,759 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:44,759 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:44,768 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:44,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:44,782 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:44,783 : INFO : EPOCH - 9 : training on 925908 raw words (688452 effective words) took 0.6s, 1081318 effective words/s\n",
      "2019-03-15 18:07:45,427 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 18:07:45,435 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 18:07:45,436 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 18:07:45,449 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 18:07:45,458 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 18:07:45,477 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 18:07:45,478 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 18:07:45,479 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 18:07:45,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 18:07:45,489 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 18:07:45,489 : INFO : EPOCH - 10 : training on 925908 raw words (688175 effective words) took 0.7s, 987113 effective words/s\n",
      "2019-03-15 18:07:45,490 : INFO : training on a 9259080 raw words (6885538 effective words) took 6.2s, 1117911 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(6885538, 9259080)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(documents, size=300,  min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13265"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 18:07:51,685 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sinatra', 0.6881442070007324),\n",
       " ('laravel', 0.679559588432312),\n",
       " ('cakephp', 0.657805323600769),\n",
       " ('rails3', 0.6141403317451477),\n",
       " ('codeigniter', 0.5789684653282166),\n",
       " ('django', 0.5771294832229614),\n",
       " ('ror', 0.5584676265716553),\n",
       " ('rspec', 0.5502299070358276),\n",
       " ('devise', 0.5450743436813354),\n",
       " ('symfony2', 0.5361107587814331)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"rails\"\n",
    "model.wv.most_similar(positive=w1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('r', 0.6384516954421997),\n",
       " ('pandas', 0.5217745900154114),\n",
       " ('python3', 0.5198883414268494),\n",
       " ('scipy', 0.5000096559524536),\n",
       " ('numpy', 0.47042790055274963),\n",
       " ('scikit-learn', 0.46748247742652893)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"python\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"numpy\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"machine\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"javascript\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
