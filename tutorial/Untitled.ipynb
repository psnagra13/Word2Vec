{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Word2Vec in Gensim and making it work!\n",
    "\n",
    "The idea behind Word2Vec is pretty simple. We are making and assumption that you can tell the meaning of a word by the company it keeps. This is analogous to the saying *show me your friends, and I'll tell who you are*. So if you have two words that have very similar neighbors (i.e. the usage context is about the same), then these words are probably quite similar in meaning or are at least highly related. For example, the words `shocked`,`appalled` and `astonished` are typically used in a similar context. \n",
    "\n",
    "In this tutorial, you will learn how to use the Gensim implementation of Word2Vec and actually get it to work! I have heard a lot of complaints about poor performance etc, but its really a combination of two things, (1) your input data and (2) your parameter settings. Note that the training algorithms in this package were ported from the [original Word2Vec implementation by Google](https://arxiv.org/pdf/1301.3781.pdf) and extended with additional functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and logging\n",
    "\n",
    "First, we start with our imports and get logging established:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports needed and set up logging\n",
    "import gzip\n",
    "import gensim \n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset \n",
    "Next, is our dataset. The secret to getting Word2Vec really working for you is to have lots and lots of text data. In this case I am going to use data from the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset. This dataset has full user reviews of cars and hotels. I have specifically concatenated all of the hotel reviews into one big file which is about 97MB compressed and 229MB uncompressed. We will use the compressed file for this tutorial. Each line in this file represents a hotel review. You can download the OpinRank Word2Vec dataset here.\n",
    "\n",
    "To avoid confusion, while gensim’s word2vec tutorial says that you need to pass it a sequence of sentences as its input, you can always pass it a whole review as a sentence (i.e. a much larger size of text), and it should not make much of a difference. \n",
    "\n",
    "Now, let's take a closer look at this data below by printing the first line. You can see that this is a pretty hefty review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n"
     ]
    }
   ],
   "source": [
    "data_file=\"reviews_data.txt.gz\"\n",
    "\n",
    "with gzip.open ('reviews_data.txt.gz', 'rb') as f:\n",
    "    for i,line in enumerate (f):\n",
    "        print(line)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read files into a list\n",
    "Now that we've had a sneak peak of our dataset, we can read it into a list so that we can pass this on to the Word2Vec model. Notice in the code below, that I am directly reading the \n",
    "compressed file. I'm also doing a mild pre-processing of the reviews using `gensim.utils.simple_preprocess (line)`. This does some basic pre-processing such as tokenization, lowercasing, etc and returns back a list of tokens (words). Documentation of this pre-processing method can be found on the official [Gensim documentation site](https://radimrehurek.com/gensim/utils.html). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:10:02,638 : INFO : reading file reviews_data.txt.gz...this may take a while\n",
      "2019-03-15 17:10:02,639 : INFO : read 0 reviews\n",
      "2019-03-15 17:10:04,166 : INFO : read 10000 reviews\n",
      "2019-03-15 17:10:05,709 : INFO : read 20000 reviews\n",
      "2019-03-15 17:10:07,474 : INFO : read 30000 reviews\n",
      "2019-03-15 17:10:09,122 : INFO : read 40000 reviews\n",
      "2019-03-15 17:10:10,928 : INFO : read 50000 reviews\n",
      "2019-03-15 17:10:12,682 : INFO : read 60000 reviews\n",
      "2019-03-15 17:10:14,338 : INFO : read 70000 reviews\n",
      "2019-03-15 17:10:15,707 : INFO : read 80000 reviews\n",
      "2019-03-15 17:10:17,147 : INFO : read 90000 reviews\n",
      "2019-03-15 17:10:18,525 : INFO : read 100000 reviews\n",
      "2019-03-15 17:10:19,893 : INFO : read 110000 reviews\n",
      "2019-03-15 17:10:21,269 : INFO : read 120000 reviews\n",
      "2019-03-15 17:10:22,693 : INFO : read 130000 reviews\n",
      "2019-03-15 17:10:24,196 : INFO : read 140000 reviews\n",
      "2019-03-15 17:10:25,581 : INFO : read 150000 reviews\n",
      "2019-03-15 17:10:27,385 : INFO : read 160000 reviews\n",
      "2019-03-15 17:10:28,775 : INFO : read 170000 reviews\n",
      "2019-03-15 17:10:30,377 : INFO : read 180000 reviews\n",
      "2019-03-15 17:10:31,966 : INFO : read 190000 reviews\n",
      "2019-03-15 17:10:33,708 : INFO : read 200000 reviews\n",
      "2019-03-15 17:10:35,243 : INFO : read 210000 reviews\n",
      "2019-03-15 17:10:36,778 : INFO : read 220000 reviews\n",
      "2019-03-15 17:10:38,365 : INFO : read 230000 reviews\n",
      "2019-03-15 17:10:39,818 : INFO : read 240000 reviews\n",
      "2019-03-15 17:10:41,947 : INFO : read 250000 reviews\n",
      "2019-03-15 17:10:42,772 : INFO : Done reading data file\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def read_input(input_file):\n",
    "    \"\"\"This method reads the input file which is in gzip format\"\"\"\n",
    "    \n",
    "    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n",
    "    \n",
    "    with gzip.open (input_file, 'rb') as f:\n",
    "        for i, line in enumerate (f): \n",
    "\n",
    "            if (i%10000==0):\n",
    "                logging.info (\"read {0} reviews\".format (i))\n",
    "            # do some pre-processing and return a list of words for each review text\n",
    "            yield gensim.utils.simple_preprocess (line)\n",
    "\n",
    "# read the tokenized reviews into a list\n",
    "# each review item becomes a serries of words\n",
    "# so this becomes a list of lists\n",
    "documents = list (read_input (data_file))\n",
    "logging.info (\"Done reading data file\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Word2Vec model\n",
    "\n",
    "Training the model is fairly straightforward. You just instantiate Word2Vec and pass the reviews that we read in the previous step (the `documents`). So, we are essentially passing on a list of lists. Where each list within the main list contains a set of tokens from a user review. Word2Vec uses all these tokens to internally create a vocabulary. And by vocabulary, I mean a set of unique words.\n",
    "\n",
    "After building the vocabulary, we just need to call `train(...)` to start training the Word2Vec model. Training on the [OpinRank](http://kavita-ganesan.com/entity-ranking-data/) dataset takes about 10 minutes so please be patient while running your code on this dataset.\n",
    "\n",
    "Behind the scenes we are actually training a simple neural network with a single hidden layer. But, we are actually not going to use the neural network after training. Instead, the goal is to learn the weights of the hidden layer. These weights are essentially the word vectors that we’re trying to learn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:10:48,365 : WARNING : consider setting layer size to a multiple of 4 for greater performance\n",
      "2019-03-15 17:10:48,366 : INFO : collecting all words and their counts\n",
      "2019-03-15 17:10:48,367 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-15 17:10:48,581 : INFO : PROGRESS: at sentence #10000, processed 1655714 words, keeping 25777 word types\n",
      "2019-03-15 17:10:48,788 : INFO : PROGRESS: at sentence #20000, processed 3317863 words, keeping 35016 word types\n",
      "2019-03-15 17:10:49,032 : INFO : PROGRESS: at sentence #30000, processed 5264072 words, keeping 47518 word types\n",
      "2019-03-15 17:10:49,258 : INFO : PROGRESS: at sentence #40000, processed 7081746 words, keeping 56675 word types\n",
      "2019-03-15 17:10:49,511 : INFO : PROGRESS: at sentence #50000, processed 9089491 words, keeping 63744 word types\n",
      "2019-03-15 17:10:49,758 : INFO : PROGRESS: at sentence #60000, processed 11013726 words, keeping 76786 word types\n",
      "2019-03-15 17:10:49,966 : INFO : PROGRESS: at sentence #70000, processed 12637528 words, keeping 83199 word types\n",
      "2019-03-15 17:10:50,171 : INFO : PROGRESS: at sentence #80000, processed 14099754 words, keeping 88459 word types\n",
      "2019-03-15 17:10:50,378 : INFO : PROGRESS: at sentence #90000, processed 15662152 words, keeping 93357 word types\n",
      "2019-03-15 17:10:50,587 : INFO : PROGRESS: at sentence #100000, processed 17164490 words, keeping 97886 word types\n",
      "2019-03-15 17:10:50,788 : INFO : PROGRESS: at sentence #110000, processed 18652295 words, keeping 102132 word types\n",
      "2019-03-15 17:10:50,981 : INFO : PROGRESS: at sentence #120000, processed 20152532 words, keeping 105923 word types\n",
      "2019-03-15 17:10:51,182 : INFO : PROGRESS: at sentence #130000, processed 21684333 words, keeping 110104 word types\n",
      "2019-03-15 17:10:51,399 : INFO : PROGRESS: at sentence #140000, processed 23330209 words, keeping 114108 word types\n",
      "2019-03-15 17:10:51,593 : INFO : PROGRESS: at sentence #150000, processed 24838757 words, keeping 118174 word types\n",
      "2019-03-15 17:10:51,789 : INFO : PROGRESS: at sentence #160000, processed 26390913 words, keeping 118670 word types\n",
      "2019-03-15 17:10:51,986 : INFO : PROGRESS: at sentence #170000, processed 27913919 words, keeping 123356 word types\n",
      "2019-03-15 17:10:52,197 : INFO : PROGRESS: at sentence #180000, processed 29535615 words, keeping 126748 word types\n",
      "2019-03-15 17:10:52,410 : INFO : PROGRESS: at sentence #190000, processed 31096462 words, keeping 129847 word types\n",
      "2019-03-15 17:10:52,627 : INFO : PROGRESS: at sentence #200000, processed 32805274 words, keeping 133255 word types\n",
      "2019-03-15 17:10:52,847 : INFO : PROGRESS: at sentence #210000, processed 34434201 words, keeping 136364 word types\n",
      "2019-03-15 17:10:53,074 : INFO : PROGRESS: at sentence #220000, processed 36083485 words, keeping 139418 word types\n",
      "2019-03-15 17:10:53,267 : INFO : PROGRESS: at sentence #230000, processed 37571765 words, keeping 142399 word types\n",
      "2019-03-15 17:10:53,469 : INFO : PROGRESS: at sentence #240000, processed 39138193 words, keeping 145232 word types\n",
      "2019-03-15 17:10:53,668 : INFO : PROGRESS: at sentence #250000, processed 40695052 words, keeping 147966 word types\n",
      "2019-03-15 17:10:53,780 : INFO : collected 150059 word types from a corpus of 41519358 raw words and 255404 sentences\n",
      "2019-03-15 17:10:53,781 : INFO : Loading a fresh vocabulary\n",
      "2019-03-15 17:10:53,902 : INFO : min_count=2 retains 70537 unique words (47% of original 150059, drops 79522)\n",
      "2019-03-15 17:10:53,902 : INFO : min_count=2 leaves 41439836 word corpus (99% of original 41519358, drops 79522)\n",
      "2019-03-15 17:10:54,050 : INFO : deleting the raw counts dictionary of 150059 items\n",
      "2019-03-15 17:10:54,054 : INFO : sample=0.001 downsamples 55 most-common words\n",
      "2019-03-15 17:10:54,054 : INFO : downsampling leaves estimated 30349251 word corpus (73.2% of prior 41439836)\n",
      "2019-03-15 17:10:54,256 : INFO : estimated required memory for 70537 words and 150 dimensions: 119912900 bytes\n",
      "2019-03-15 17:10:54,257 : INFO : resetting layer weights\n",
      "2019-03-15 17:10:54,856 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-15 17:10:55,861 : INFO : EPOCH 1 - PROGRESS: at 3.96% examples, 1226109 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:10:56,866 : INFO : EPOCH 1 - PROGRESS: at 8.81% examples, 1361621 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:10:57,867 : INFO : EPOCH 1 - PROGRESS: at 12.77% examples, 1405652 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:10:58,875 : INFO : EPOCH 1 - PROGRESS: at 17.23% examples, 1435461 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:10:59,883 : INFO : EPOCH 1 - PROGRESS: at 21.52% examples, 1451138 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:00,892 : INFO : EPOCH 1 - PROGRESS: at 25.96% examples, 1466108 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:01,902 : INFO : EPOCH 1 - PROGRESS: at 31.65% examples, 1476265 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:11:02,932 : INFO : EPOCH 1 - PROGRESS: at 36.68% examples, 1468498 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:03,933 : INFO : EPOCH 1 - PROGRESS: at 40.90% examples, 1438217 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:04,934 : INFO : EPOCH 1 - PROGRESS: at 46.35% examples, 1443411 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:05,939 : INFO : EPOCH 1 - PROGRESS: at 51.42% examples, 1444428 words/s, in_qsize 18, out_qsize 4\n",
      "2019-03-15 17:11:06,941 : INFO : EPOCH 1 - PROGRESS: at 56.49% examples, 1449371 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:07,947 : INFO : EPOCH 1 - PROGRESS: at 61.66% examples, 1453070 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:11:08,952 : INFO : EPOCH 1 - PROGRESS: at 66.51% examples, 1447013 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:11:09,962 : INFO : EPOCH 1 - PROGRESS: at 70.41% examples, 1428669 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:11:10,967 : INFO : EPOCH 1 - PROGRESS: at 75.08% examples, 1424972 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:11,981 : INFO : EPOCH 1 - PROGRESS: at 79.99% examples, 1431127 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:13,000 : INFO : EPOCH 1 - PROGRESS: at 85.01% examples, 1435534 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:11:14,002 : INFO : EPOCH 1 - PROGRESS: at 90.33% examples, 1438782 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:15,026 : INFO : EPOCH 1 - PROGRESS: at 95.31% examples, 1438513 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-15 17:11:15,883 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:11:15,883 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:11:15,889 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:11:15,897 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:11:15,902 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:11:15,903 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:11:15,905 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:11:15,906 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:11:15,907 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:11:15,911 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:11:15,912 : INFO : EPOCH - 1 : training on 41519358 raw words (30349901 effective words) took 21.1s, 1441686 effective words/s\n",
      "2019-03-15 17:11:16,925 : INFO : EPOCH 2 - PROGRESS: at 4.89% examples, 1502606 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:17,930 : INFO : EPOCH 2 - PROGRESS: at 9.68% examples, 1512008 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:18,933 : INFO : EPOCH 2 - PROGRESS: at 13.48% examples, 1472304 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:19,957 : INFO : EPOCH 2 - PROGRESS: at 17.59% examples, 1461495 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:20,967 : INFO : EPOCH 2 - PROGRESS: at 21.84% examples, 1468261 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:11:21,974 : INFO : EPOCH 2 - PROGRESS: at 26.30% examples, 1476246 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:22,977 : INFO : EPOCH 2 - PROGRESS: at 31.57% examples, 1469781 words/s, in_qsize 20, out_qsize 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:11:23,995 : INFO : EPOCH 2 - PROGRESS: at 36.82% examples, 1473200 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:25,002 : INFO : EPOCH 2 - PROGRESS: at 42.33% examples, 1477839 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:11:26,008 : INFO : EPOCH 2 - PROGRESS: at 47.56% examples, 1477010 words/s, in_qsize 17, out_qsize 1\n",
      "2019-03-15 17:11:27,008 : INFO : EPOCH 2 - PROGRESS: at 52.64% examples, 1477483 words/s, in_qsize 13, out_qsize 6\n",
      "2019-03-15 17:11:28,009 : INFO : EPOCH 2 - PROGRESS: at 57.88% examples, 1481851 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:11:29,010 : INFO : EPOCH 2 - PROGRESS: at 62.99% examples, 1481877 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:30,018 : INFO : EPOCH 2 - PROGRESS: at 68.30% examples, 1483429 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:11:31,018 : INFO : EPOCH 2 - PROGRESS: at 73.15% examples, 1482078 words/s, in_qsize 18, out_qsize 0\n",
      "2019-03-15 17:11:32,029 : INFO : EPOCH 2 - PROGRESS: at 77.83% examples, 1479432 words/s, in_qsize 15, out_qsize 4\n",
      "2019-03-15 17:11:33,033 : INFO : EPOCH 2 - PROGRESS: at 82.46% examples, 1475529 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:34,085 : INFO : EPOCH 2 - PROGRESS: at 87.06% examples, 1465702 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:35,097 : INFO : EPOCH 2 - PROGRESS: at 92.25% examples, 1464762 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:36,110 : INFO : EPOCH 2 - PROGRESS: at 96.21% examples, 1449562 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:11:36,877 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:11:36,878 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:11:36,886 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:11:36,887 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:11:36,888 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:11:36,908 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:11:36,910 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:11:36,913 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:11:36,918 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:11:36,920 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:11:36,920 : INFO : EPOCH - 2 : training on 41519358 raw words (30350137 effective words) took 21.0s, 1445256 effective words/s\n",
      "2019-03-15 17:11:37,926 : INFO : EPOCH 3 - PROGRESS: at 3.66% examples, 1132074 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:38,926 : INFO : EPOCH 3 - PROGRESS: at 7.97% examples, 1235497 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:39,971 : INFO : EPOCH 3 - PROGRESS: at 11.21% examples, 1191621 words/s, in_qsize 16, out_qsize 3\n",
      "2019-03-15 17:11:40,985 : INFO : EPOCH 3 - PROGRESS: at 14.73% examples, 1200612 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:41,990 : INFO : EPOCH 3 - PROGRESS: at 18.20% examples, 1208556 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:11:42,999 : INFO : EPOCH 3 - PROGRESS: at 21.89% examples, 1223489 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:11:44,000 : INFO : EPOCH 3 - PROGRESS: at 25.63% examples, 1237499 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:45,004 : INFO : EPOCH 3 - PROGRESS: at 30.90% examples, 1263467 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:46,013 : INFO : EPOCH 3 - PROGRESS: at 35.61% examples, 1271558 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:47,015 : INFO : EPOCH 3 - PROGRESS: at 40.29% examples, 1275873 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:11:48,024 : INFO : EPOCH 3 - PROGRESS: at 45.66% examples, 1291667 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:49,027 : INFO : EPOCH 3 - PROGRESS: at 50.77% examples, 1306742 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:50,032 : INFO : EPOCH 3 - PROGRESS: at 55.74% examples, 1318603 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:51,048 : INFO : EPOCH 3 - PROGRESS: at 60.95% examples, 1330864 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:52,062 : INFO : EPOCH 3 - PROGRESS: at 66.26% examples, 1342586 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:53,063 : INFO : EPOCH 3 - PROGRESS: at 71.10% examples, 1350788 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:54,078 : INFO : EPOCH 3 - PROGRESS: at 76.20% examples, 1358940 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:55,079 : INFO : EPOCH 3 - PROGRESS: at 80.90% examples, 1364899 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:56,090 : INFO : EPOCH 3 - PROGRESS: at 85.94% examples, 1372807 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:57,092 : INFO : EPOCH 3 - PROGRESS: at 91.17% examples, 1377257 words/s, in_qsize 18, out_qsize 2\n",
      "2019-03-15 17:11:58,103 : INFO : EPOCH 3 - PROGRESS: at 96.35% examples, 1383485 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:11:58,762 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:11:58,777 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:11:58,797 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:11:58,800 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:11:58,803 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:11:58,810 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:11:58,813 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:11:58,815 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:11:58,821 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:11:58,822 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:11:58,822 : INFO : EPOCH - 3 : training on 41519358 raw words (30345617 effective words) took 21.9s, 1385863 effective words/s\n",
      "2019-03-15 17:11:59,832 : INFO : EPOCH 4 - PROGRESS: at 4.84% examples, 1478420 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:00,834 : INFO : EPOCH 4 - PROGRESS: at 9.46% examples, 1478404 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:01,865 : INFO : EPOCH 4 - PROGRESS: at 13.58% examples, 1471125 words/s, in_qsize 20, out_qsize 5\n",
      "2019-03-15 17:12:02,866 : INFO : EPOCH 4 - PROGRESS: at 17.88% examples, 1485265 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:03,868 : INFO : EPOCH 4 - PROGRESS: at 22.03% examples, 1483780 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:04,869 : INFO : EPOCH 4 - PROGRESS: at 26.39% examples, 1482434 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:05,873 : INFO : EPOCH 4 - PROGRESS: at 31.74% examples, 1477803 words/s, in_qsize 18, out_qsize 3\n",
      "2019-03-15 17:12:06,879 : INFO : EPOCH 4 - PROGRESS: at 36.94% examples, 1482282 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-15 17:12:07,880 : INFO : EPOCH 4 - PROGRESS: at 42.42% examples, 1486238 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:12:08,883 : INFO : EPOCH 4 - PROGRESS: at 47.74% examples, 1486414 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:09,897 : INFO : EPOCH 4 - PROGRESS: at 52.90% examples, 1488064 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:10,904 : INFO : EPOCH 4 - PROGRESS: at 58.05% examples, 1487178 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:11,907 : INFO : EPOCH 4 - PROGRESS: at 63.29% examples, 1488381 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:12,910 : INFO : EPOCH 4 - PROGRESS: at 68.36% examples, 1486424 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:13,935 : INFO : EPOCH 4 - PROGRESS: at 73.13% examples, 1480998 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:12:14,936 : INFO : EPOCH 4 - PROGRESS: at 77.89% examples, 1480957 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-15 17:12:15,941 : INFO : EPOCH 4 - PROGRESS: at 82.79% examples, 1480728 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:12:16,941 : INFO : EPOCH 4 - PROGRESS: at 87.80% examples, 1480813 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:12:17,946 : INFO : EPOCH 4 - PROGRESS: at 93.07% examples, 1482614 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:12:18,950 : INFO : EPOCH 4 - PROGRESS: at 98.13% examples, 1481833 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:19,277 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:12:19,281 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:12:19,282 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:12:19,282 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:12:19,291 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:12:19,292 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:12:19,304 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:12:19,305 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:12:19,307 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:12:19,312 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:12:19,312 : INFO : EPOCH - 4 : training on 41519358 raw words (30350223 effective words) took 20.5s, 1481505 effective words/s\n",
      "2019-03-15 17:12:20,317 : INFO : EPOCH 5 - PROGRESS: at 4.83% examples, 1486571 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:12:21,319 : INFO : EPOCH 5 - PROGRESS: at 9.59% examples, 1503412 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:22,334 : INFO : EPOCH 5 - PROGRESS: at 13.64% examples, 1490895 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:23,344 : INFO : EPOCH 5 - PROGRESS: at 17.90% examples, 1491523 words/s, in_qsize 15, out_qsize 4\n",
      "2019-03-15 17:12:24,349 : INFO : EPOCH 5 - PROGRESS: at 22.11% examples, 1492565 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:12:25,355 : INFO : EPOCH 5 - PROGRESS: at 26.68% examples, 1496742 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:26,358 : INFO : EPOCH 5 - PROGRESS: at 32.11% examples, 1495668 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:27,358 : INFO : EPOCH 5 - PROGRESS: at 37.30% examples, 1497332 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:28,364 : INFO : EPOCH 5 - PROGRESS: at 42.69% examples, 1496292 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:29,380 : INFO : EPOCH 5 - PROGRESS: at 47.59% examples, 1481265 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:12:30,385 : INFO : EPOCH 5 - PROGRESS: at 52.68% examples, 1481339 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:12:31,387 : INFO : EPOCH 5 - PROGRESS: at 57.89% examples, 1484026 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:32,395 : INFO : EPOCH 5 - PROGRESS: at 61.71% examples, 1454877 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:12:33,407 : INFO : EPOCH 5 - PROGRESS: at 65.75% examples, 1431243 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:34,423 : INFO : EPOCH 5 - PROGRESS: at 69.92% examples, 1417719 words/s, in_qsize 17, out_qsize 3\n",
      "2019-03-15 17:12:35,433 : INFO : EPOCH 5 - PROGRESS: at 74.74% examples, 1416491 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:36,434 : INFO : EPOCH 5 - PROGRESS: at 79.40% examples, 1420294 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:37,439 : INFO : EPOCH 5 - PROGRESS: at 84.23% examples, 1422555 words/s, in_qsize 15, out_qsize 4\n",
      "2019-03-15 17:12:38,443 : INFO : EPOCH 5 - PROGRESS: at 89.20% examples, 1423256 words/s, in_qsize 19, out_qsize 3\n",
      "2019-03-15 17:12:39,447 : INFO : EPOCH 5 - PROGRESS: at 93.72% examples, 1417484 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:12:40,451 : INFO : EPOCH 5 - PROGRESS: at 98.54% examples, 1416406 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:12:40,694 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:12:40,720 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:12:40,722 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:12:40,723 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:12:40,725 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:12:40,726 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:12:40,726 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:12:40,727 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:12:40,727 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:12:40,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:12:40,728 : INFO : EPOCH - 5 : training on 41519358 raw words (30349384 effective words) took 21.4s, 1417433 effective words/s\n",
      "2019-03-15 17:12:40,729 : INFO : training on a 207596790 raw words (151745262 effective words) took 105.9s, 1433287 effective words/s\n",
      "2019-03-15 17:12:40,729 : WARNING : Effective 'alpha' higher than previous training cycles\n",
      "2019-03-15 17:12:40,729 : INFO : training model with 10 workers on 70537 vocabulary and 150 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-03-15 17:12:41,740 : INFO : EPOCH 1 - PROGRESS: at 4.61% examples, 1418869 words/s, in_qsize 20, out_qsize 3\n",
      "2019-03-15 17:12:42,747 : INFO : EPOCH 1 - PROGRESS: at 9.43% examples, 1466499 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:43,753 : INFO : EPOCH 1 - PROGRESS: at 13.51% examples, 1473348 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:44,757 : INFO : EPOCH 1 - PROGRESS: at 17.70% examples, 1478491 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:12:45,770 : INFO : EPOCH 1 - PROGRESS: at 21.84% examples, 1472076 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:46,780 : INFO : EPOCH 1 - PROGRESS: at 26.17% examples, 1474184 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:12:47,784 : INFO : EPOCH 1 - PROGRESS: at 31.62% examples, 1474025 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:48,786 : INFO : EPOCH 1 - PROGRESS: at 36.80% examples, 1477127 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:49,790 : INFO : EPOCH 1 - PROGRESS: at 42.28% examples, 1481042 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:50,796 : INFO : EPOCH 1 - PROGRESS: at 47.60% examples, 1482839 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:51,801 : INFO : EPOCH 1 - PROGRESS: at 52.71% examples, 1483311 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:12:52,803 : INFO : EPOCH 1 - PROGRESS: at 57.81% examples, 1482973 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:53,810 : INFO : EPOCH 1 - PROGRESS: at 63.15% examples, 1486652 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:54,819 : INFO : EPOCH 1 - PROGRESS: at 68.25% examples, 1484272 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:55,834 : INFO : EPOCH 1 - PROGRESS: at 73.22% examples, 1483713 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:56,836 : INFO : EPOCH 1 - PROGRESS: at 78.03% examples, 1484814 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:57,847 : INFO : EPOCH 1 - PROGRESS: at 82.82% examples, 1481213 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:12:58,849 : INFO : EPOCH 1 - PROGRESS: at 86.76% examples, 1465590 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:12:59,871 : INFO : EPOCH 1 - PROGRESS: at 91.28% examples, 1453774 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:00,884 : INFO : EPOCH 1 - PROGRESS: at 96.00% examples, 1449533 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:13:01,697 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:13:01,699 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:13:01,713 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:13:01,716 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:13:01,716 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:13:01,717 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:13:01,717 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:13:01,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:13:01,731 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:13:01,732 : INFO : worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:13:01,733 : INFO : EPOCH - 1 : training on 41519358 raw words (30351281 effective words) took 21.0s, 1445619 effective words/s\n",
      "2019-03-15 17:13:02,739 : INFO : EPOCH 2 - PROGRESS: at 4.07% examples, 1255211 words/s, in_qsize 18, out_qsize 5\n",
      "2019-03-15 17:13:03,741 : INFO : EPOCH 2 - PROGRESS: at 8.62% examples, 1331641 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:04,741 : INFO : EPOCH 2 - PROGRESS: at 12.36% examples, 1356673 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:05,752 : INFO : EPOCH 2 - PROGRESS: at 16.56% examples, 1374259 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:06,755 : INFO : EPOCH 2 - PROGRESS: at 19.91% examples, 1350178 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:07,759 : INFO : EPOCH 2 - PROGRESS: at 23.10% examples, 1311898 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:08,771 : INFO : EPOCH 2 - PROGRESS: at 27.26% examples, 1307713 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:09,775 : INFO : EPOCH 2 - PROGRESS: at 32.11% examples, 1310320 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:10,785 : INFO : EPOCH 2 - PROGRESS: at 36.71% examples, 1310740 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:11,790 : INFO : EPOCH 2 - PROGRESS: at 40.84% examples, 1296501 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:12,794 : INFO : EPOCH 2 - PROGRESS: at 45.51% examples, 1292300 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:13,797 : INFO : EPOCH 2 - PROGRESS: at 50.39% examples, 1302368 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:13:14,797 : INFO : EPOCH 2 - PROGRESS: at 55.11% examples, 1311346 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:15,804 : INFO : EPOCH 2 - PROGRESS: at 59.58% examples, 1308437 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:16,805 : INFO : EPOCH 2 - PROGRESS: at 64.60% examples, 1314130 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:17,811 : INFO : EPOCH 2 - PROGRESS: at 69.49% examples, 1324363 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:13:18,826 : INFO : EPOCH 2 - PROGRESS: at 74.24% examples, 1327530 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:19,845 : INFO : EPOCH 2 - PROGRESS: at 77.97% examples, 1318772 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:20,863 : INFO : EPOCH 2 - PROGRESS: at 81.68% examples, 1308046 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:21,866 : INFO : EPOCH 2 - PROGRESS: at 86.18% examples, 1311119 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:22,885 : INFO : EPOCH 2 - PROGRESS: at 90.99% examples, 1311509 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:23,886 : INFO : EPOCH 2 - PROGRESS: at 96.11% examples, 1320040 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:24,619 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:13:24,622 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:13:24,625 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:13:24,625 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:13:24,630 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:13:24,637 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:13:24,639 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:13:24,641 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:13:24,643 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:13:24,652 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:13:24,654 : INFO : EPOCH - 2 : training on 41519358 raw words (30348440 effective words) took 22.9s, 1324413 effective words/s\n",
      "2019-03-15 17:13:25,669 : INFO : EPOCH 3 - PROGRESS: at 4.72% examples, 1446218 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:26,678 : INFO : EPOCH 3 - PROGRESS: at 9.35% examples, 1446288 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:27,681 : INFO : EPOCH 3 - PROGRESS: at 13.36% examples, 1454307 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:28,683 : INFO : EPOCH 3 - PROGRESS: at 17.52% examples, 1459325 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:13:29,687 : INFO : EPOCH 3 - PROGRESS: at 21.67% examples, 1461280 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:13:30,691 : INFO : EPOCH 3 - PROGRESS: at 25.77% examples, 1459015 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:31,696 : INFO : EPOCH 3 - PROGRESS: at 31.16% examples, 1459992 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:32,699 : INFO : EPOCH 3 - PROGRESS: at 36.25% examples, 1459068 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:33,706 : INFO : EPOCH 3 - PROGRESS: at 41.42% examples, 1456790 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:34,713 : INFO : EPOCH 3 - PROGRESS: at 46.73% examples, 1457774 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:35,718 : INFO : EPOCH 3 - PROGRESS: at 51.70% examples, 1454774 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:36,722 : INFO : EPOCH 3 - PROGRESS: at 56.64% examples, 1455183 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:13:37,746 : INFO : EPOCH 3 - PROGRESS: at 61.61% examples, 1451832 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:38,751 : INFO : EPOCH 3 - PROGRESS: at 66.56% examples, 1448434 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:39,768 : INFO : EPOCH 3 - PROGRESS: at 71.25% examples, 1445223 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:13:40,768 : INFO : EPOCH 3 - PROGRESS: at 76.13% examples, 1445786 words/s, in_qsize 15, out_qsize 4\n",
      "2019-03-15 17:13:41,775 : INFO : EPOCH 3 - PROGRESS: at 80.85% examples, 1446964 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:13:42,799 : INFO : EPOCH 3 - PROGRESS: at 85.67% examples, 1446541 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:13:43,804 : INFO : EPOCH 3 - PROGRESS: at 90.58% examples, 1442345 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:13:44,805 : INFO : EPOCH 3 - PROGRESS: at 94.77% examples, 1431575 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:45,814 : INFO : EPOCH 3 - PROGRESS: at 99.38% examples, 1426319 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:13:45,909 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:13:45,919 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:13:45,937 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:13:45,948 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:13:45,950 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:13:45,964 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:13:45,968 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:13:45,970 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:13:45,975 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:13:45,981 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:13:45,982 : INFO : EPOCH - 3 : training on 41519358 raw words (30347534 effective words) took 21.3s, 1423340 effective words/s\n",
      "2019-03-15 17:13:46,986 : INFO : EPOCH 4 - PROGRESS: at 4.51% examples, 1392191 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:13:47,988 : INFO : EPOCH 4 - PROGRESS: at 9.22% examples, 1435864 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:48,988 : INFO : EPOCH 4 - PROGRESS: at 13.14% examples, 1441430 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:13:50,007 : INFO : EPOCH 4 - PROGRESS: at 17.31% examples, 1440479 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:13:51,012 : INFO : EPOCH 4 - PROGRESS: at 20.67% examples, 1409119 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:52,018 : INFO : EPOCH 4 - PROGRESS: at 24.40% examples, 1392870 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:53,025 : INFO : EPOCH 4 - PROGRESS: at 29.76% examples, 1404893 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:54,028 : INFO : EPOCH 4 - PROGRESS: at 34.81% examples, 1409257 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:55,032 : INFO : EPOCH 4 - PROGRESS: at 39.92% examples, 1412139 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:13:56,033 : INFO : EPOCH 4 - PROGRESS: at 45.24% examples, 1415602 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:13:57,041 : INFO : EPOCH 4 - PROGRESS: at 50.23% examples, 1416196 words/s, in_qsize 19, out_qsize 7\n",
      "2019-03-15 17:13:58,041 : INFO : EPOCH 4 - PROGRESS: at 55.10% examples, 1419965 words/s, in_qsize 20, out_qsize 4\n",
      "2019-03-15 17:13:59,041 : INFO : EPOCH 4 - PROGRESS: at 60.14% examples, 1422484 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:00,042 : INFO : EPOCH 4 - PROGRESS: at 65.24% examples, 1422459 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:14:01,050 : INFO : EPOCH 4 - PROGRESS: at 69.97% examples, 1423143 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:02,056 : INFO : EPOCH 4 - PROGRESS: at 74.90% examples, 1424101 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:03,059 : INFO : EPOCH 4 - PROGRESS: at 79.48% examples, 1425667 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:14:04,068 : INFO : EPOCH 4 - PROGRESS: at 84.39% examples, 1428896 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:05,087 : INFO : EPOCH 4 - PROGRESS: at 89.42% examples, 1429473 words/s, in_qsize 19, out_qsize 4\n",
      "2019-03-15 17:14:06,082 : INFO : EPOCH 4 - PROGRESS: at 94.46% examples, 1430717 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:07,084 : INFO : EPOCH 4 - PROGRESS: at 99.59% examples, 1433137 words/s, in_qsize 16, out_qsize 0\n",
      "2019-03-15 17:14:07,142 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:14:07,142 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:14:07,143 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:14:07,146 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:14:07,150 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:14:07,162 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:14:07,163 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:14:07,171 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:14:07,178 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:14:07,185 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:14:07,186 : INFO : EPOCH - 4 : training on 41519358 raw words (30348665 effective words) took 21.2s, 1431528 effective words/s\n",
      "2019-03-15 17:14:08,192 : INFO : EPOCH 5 - PROGRESS: at 4.50% examples, 1390452 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:09,203 : INFO : EPOCH 5 - PROGRESS: at 9.12% examples, 1414400 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:10,210 : INFO : EPOCH 5 - PROGRESS: at 13.08% examples, 1427917 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:11,217 : INFO : EPOCH 5 - PROGRESS: at 17.28% examples, 1436417 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:14:12,220 : INFO : EPOCH 5 - PROGRESS: at 20.99% examples, 1433921 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:14:13,221 : INFO : EPOCH 5 - PROGRESS: at 24.03% examples, 1370572 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:14,239 : INFO : EPOCH 5 - PROGRESS: at 27.11% examples, 1298570 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:15,247 : INFO : EPOCH 5 - PROGRESS: at 31.81% examples, 1296580 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:16,252 : INFO : EPOCH 5 - PROGRESS: at 36.55% examples, 1304026 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:17,252 : INFO : EPOCH 5 - PROGRESS: at 41.50% examples, 1311298 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:18,252 : INFO : EPOCH 5 - PROGRESS: at 45.24% examples, 1285844 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:14:19,253 : INFO : EPOCH 5 - PROGRESS: at 49.68% examples, 1285455 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:14:20,260 : INFO : EPOCH 5 - PROGRESS: at 54.52% examples, 1297734 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:14:21,273 : INFO : EPOCH 5 - PROGRESS: at 59.72% examples, 1309546 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:22,282 : INFO : EPOCH 5 - PROGRESS: at 65.04% examples, 1320219 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:23,288 : INFO : EPOCH 5 - PROGRESS: at 69.79% examples, 1327883 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:24,290 : INFO : EPOCH 5 - PROGRESS: at 74.76% examples, 1335636 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:25,306 : INFO : EPOCH 5 - PROGRESS: at 79.31% examples, 1340566 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:26,311 : INFO : EPOCH 5 - PROGRESS: at 84.14% examples, 1346946 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:27,311 : INFO : EPOCH 5 - PROGRESS: at 89.08% examples, 1351180 words/s, in_qsize 17, out_qsize 3\n",
      "2019-03-15 17:14:28,315 : INFO : EPOCH 5 - PROGRESS: at 94.18% examples, 1357027 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:29,340 : INFO : EPOCH 5 - PROGRESS: at 99.13% examples, 1359006 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:29,463 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:14:29,470 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:14:29,472 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:14:29,472 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:14:29,482 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:14:29,485 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:14:29,486 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:14:29,487 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:14:29,490 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:14:29,500 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:14:29,501 : INFO : EPOCH - 5 : training on 41519358 raw words (30350876 effective words) took 22.3s, 1360340 effective words/s\n",
      "2019-03-15 17:14:30,519 : INFO : EPOCH 6 - PROGRESS: at 4.60% examples, 1407887 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:31,526 : INFO : EPOCH 6 - PROGRESS: at 9.37% examples, 1450104 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:32,529 : INFO : EPOCH 6 - PROGRESS: at 13.21% examples, 1440014 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:33,532 : INFO : EPOCH 6 - PROGRESS: at 17.40% examples, 1448438 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:34,538 : INFO : EPOCH 6 - PROGRESS: at 20.87% examples, 1423871 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:35,571 : INFO : EPOCH 6 - PROGRESS: at 24.67% examples, 1404649 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:36,563 : INFO : EPOCH 6 - PROGRESS: at 28.77% examples, 1362104 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:37,576 : INFO : EPOCH 6 - PROGRESS: at 32.47% examples, 1317950 words/s, in_qsize 17, out_qsize 6\n",
      "2019-03-15 17:14:38,578 : INFO : EPOCH 6 - PROGRESS: at 35.06% examples, 1257548 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:14:39,588 : INFO : EPOCH 6 - PROGRESS: at 37.70% examples, 1205720 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:14:40,613 : INFO : EPOCH 6 - PROGRESS: at 40.51% examples, 1165769 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:14:41,615 : INFO : EPOCH 6 - PROGRESS: at 43.26% examples, 1130620 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:42,626 : INFO : EPOCH 6 - PROGRESS: at 46.12% examples, 1102262 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:14:43,662 : INFO : EPOCH 6 - PROGRESS: at 49.10% examples, 1082622 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:14:44,674 : INFO : EPOCH 6 - PROGRESS: at 51.98% examples, 1066770 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:45,675 : INFO : EPOCH 6 - PROGRESS: at 55.08% examples, 1058308 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:46,686 : INFO : EPOCH 6 - PROGRESS: at 59.18% examples, 1064538 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:47,694 : INFO : EPOCH 6 - PROGRESS: at 63.26% examples, 1070074 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:48,698 : INFO : EPOCH 6 - PROGRESS: at 66.22% examples, 1058292 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:14:49,743 : INFO : EPOCH 6 - PROGRESS: at 69.26% examples, 1048125 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:14:50,760 : INFO : EPOCH 6 - PROGRESS: at 73.37% examples, 1055439 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:51,762 : INFO : EPOCH 6 - PROGRESS: at 78.09% examples, 1074611 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:14:52,787 : INFO : EPOCH 6 - PROGRESS: at 82.55% examples, 1085431 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:53,795 : INFO : EPOCH 6 - PROGRESS: at 87.04% examples, 1095803 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:54,795 : INFO : EPOCH 6 - PROGRESS: at 90.65% examples, 1092724 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:55,800 : INFO : EPOCH 6 - PROGRESS: at 94.35% examples, 1092117 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:56,805 : INFO : EPOCH 6 - PROGRESS: at 99.23% examples, 1103746 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:14:56,935 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:14:56,937 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:14:56,937 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:14:56,938 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:14:56,953 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:14:56,957 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:14:56,967 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:14:56,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:14:56,990 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:14:56,991 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:14:56,991 : INFO : EPOCH - 6 : training on 41519358 raw words (30347191 effective words) took 27.5s, 1104248 effective words/s\n",
      "2019-03-15 17:14:58,001 : INFO : EPOCH 7 - PROGRESS: at 4.71% examples, 1446337 words/s, in_qsize 16, out_qsize 3\n",
      "2019-03-15 17:14:59,005 : INFO : EPOCH 7 - PROGRESS: at 8.35% examples, 1288691 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:15:00,017 : INFO : EPOCH 7 - PROGRESS: at 10.78% examples, 1147226 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:01,025 : INFO : EPOCH 7 - PROGRESS: at 13.09% examples, 1070759 words/s, in_qsize 18, out_qsize 3\n",
      "2019-03-15 17:15:02,026 : INFO : EPOCH 7 - PROGRESS: at 15.78% examples, 1038320 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:15:03,028 : INFO : EPOCH 7 - PROGRESS: at 18.77% examples, 1048493 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:15:04,044 : INFO : EPOCH 7 - PROGRESS: at 22.22% examples, 1071813 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:05,052 : INFO : EPOCH 7 - PROGRESS: at 25.62% examples, 1086844 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:06,062 : INFO : EPOCH 7 - PROGRESS: at 30.23% examples, 1107594 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:15:07,064 : INFO : EPOCH 7 - PROGRESS: at 34.09% examples, 1104789 words/s, in_qsize 18, out_qsize 2\n",
      "2019-03-15 17:15:08,066 : INFO : EPOCH 7 - PROGRESS: at 37.53% examples, 1093582 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:09,068 : INFO : EPOCH 7 - PROGRESS: at 42.73% examples, 1122594 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:15:10,076 : INFO : EPOCH 7 - PROGRESS: at 47.96% examples, 1147981 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:11,082 : INFO : EPOCH 7 - PROGRESS: at 52.91% examples, 1169760 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:15:12,087 : INFO : EPOCH 7 - PROGRESS: at 57.83% examples, 1186136 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:13,091 : INFO : EPOCH 7 - PROGRESS: at 62.85% examples, 1202563 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:14,109 : INFO : EPOCH 7 - PROGRESS: at 67.98% examples, 1216662 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:15,117 : INFO : EPOCH 7 - PROGRESS: at 72.87% examples, 1230555 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:16,117 : INFO : EPOCH 7 - PROGRESS: at 77.63% examples, 1243302 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:17,128 : INFO : EPOCH 7 - PROGRESS: at 80.74% examples, 1228421 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:18,140 : INFO : EPOCH 7 - PROGRESS: at 83.59% examples, 1210218 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:19,157 : INFO : EPOCH 7 - PROGRESS: at 86.33% examples, 1192639 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:15:20,157 : INFO : EPOCH 7 - PROGRESS: at 90.39% examples, 1190086 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:15:21,160 : INFO : EPOCH 7 - PROGRESS: at 95.33% examples, 1200165 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:15:22,168 : INFO : EPOCH 7 - PROGRESS: at 99.37% examples, 1198766 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:22,279 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:15:22,313 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:15:22,317 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:15:22,326 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:15:22,330 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:15:22,341 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:15:22,352 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:15:22,358 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:15:22,363 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:15:22,365 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:15:22,365 : INFO : EPOCH - 7 : training on 41519358 raw words (30348561 effective words) took 25.4s, 1196387 effective words/s\n",
      "2019-03-15 17:15:23,382 : INFO : EPOCH 8 - PROGRESS: at 3.73% examples, 1140934 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:15:24,386 : INFO : EPOCH 8 - PROGRESS: at 7.60% examples, 1165582 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:25,400 : INFO : EPOCH 8 - PROGRESS: at 11.60% examples, 1245262 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:26,428 : INFO : EPOCH 8 - PROGRESS: at 15.10% examples, 1234667 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:15:27,428 : INFO : EPOCH 8 - PROGRESS: at 18.93% examples, 1262367 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:28,429 : INFO : EPOCH 8 - PROGRESS: at 22.81% examples, 1282185 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:29,436 : INFO : EPOCH 8 - PROGRESS: at 27.22% examples, 1299440 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:15:30,443 : INFO : EPOCH 8 - PROGRESS: at 32.20% examples, 1307100 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:15:31,451 : INFO : EPOCH 8 - PROGRESS: at 36.13% examples, 1287722 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:15:32,452 : INFO : EPOCH 8 - PROGRESS: at 39.96% examples, 1268385 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:15:33,466 : INFO : EPOCH 8 - PROGRESS: at 44.76% examples, 1269523 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:15:34,476 : INFO : EPOCH 8 - PROGRESS: at 49.12% examples, 1266538 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:35,481 : INFO : EPOCH 8 - PROGRESS: at 53.81% examples, 1277546 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:36,488 : INFO : EPOCH 8 - PROGRESS: at 58.91% examples, 1289812 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:37,511 : INFO : EPOCH 8 - PROGRESS: at 63.04% examples, 1281580 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:38,533 : INFO : EPOCH 8 - PROGRESS: at 66.77% examples, 1266244 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:15:39,537 : INFO : EPOCH 8 - PROGRESS: at 71.09% examples, 1269380 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:40,540 : INFO : EPOCH 8 - PROGRESS: at 74.79% examples, 1257570 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:15:41,549 : INFO : EPOCH 8 - PROGRESS: at 78.57% examples, 1254500 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:42,563 : INFO : EPOCH 8 - PROGRESS: at 81.72% examples, 1239250 words/s, in_qsize 19, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:15:43,568 : INFO : EPOCH 8 - PROGRESS: at 84.39% examples, 1218901 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-15 17:15:44,577 : INFO : EPOCH 8 - PROGRESS: at 87.23% examples, 1201040 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:45,579 : INFO : EPOCH 8 - PROGRESS: at 90.62% examples, 1190326 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:46,609 : INFO : EPOCH 8 - PROGRESS: at 95.44% examples, 1197644 words/s, in_qsize 20, out_qsize 3\n",
      "2019-03-15 17:15:47,460 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:15:47,463 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:15:47,473 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:15:47,474 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:15:47,478 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:15:47,498 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:15:47,532 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:15:47,534 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:15:47,536 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:15:47,536 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:15:47,537 : INFO : EPOCH - 8 : training on 41519358 raw words (30348772 effective words) took 25.2s, 1205909 effective words/s\n",
      "2019-03-15 17:15:48,546 : INFO : EPOCH 9 - PROGRESS: at 4.06% examples, 1248271 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:49,561 : INFO : EPOCH 9 - PROGRESS: at 8.58% examples, 1316153 words/s, in_qsize 19, out_qsize 4\n",
      "2019-03-15 17:15:50,566 : INFO : EPOCH 9 - PROGRESS: at 12.45% examples, 1358244 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-15 17:15:51,582 : INFO : EPOCH 9 - PROGRESS: at 16.63% examples, 1370599 words/s, in_qsize 20, out_qsize 5\n",
      "2019-03-15 17:15:52,586 : INFO : EPOCH 9 - PROGRESS: at 20.39% examples, 1379169 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:53,596 : INFO : EPOCH 9 - PROGRESS: at 24.31% examples, 1381524 words/s, in_qsize 16, out_qsize 3\n",
      "2019-03-15 17:15:54,603 : INFO : EPOCH 9 - PROGRESS: at 29.46% examples, 1387801 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:55,615 : INFO : EPOCH 9 - PROGRESS: at 33.12% examples, 1339855 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:15:56,619 : INFO : EPOCH 9 - PROGRESS: at 35.93% examples, 1283330 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:15:57,620 : INFO : EPOCH 9 - PROGRESS: at 39.09% examples, 1242946 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:15:58,635 : INFO : EPOCH 9 - PROGRESS: at 42.00% examples, 1200803 words/s, in_qsize 14, out_qsize 5\n",
      "2019-03-15 17:15:59,654 : INFO : EPOCH 9 - PROGRESS: at 45.11% examples, 1172241 words/s, in_qsize 19, out_qsize 4\n",
      "2019-03-15 17:16:00,654 : INFO : EPOCH 9 - PROGRESS: at 48.20% examples, 1150852 words/s, in_qsize 18, out_qsize 4\n",
      "2019-03-15 17:16:01,664 : INFO : EPOCH 9 - PROGRESS: at 52.71% examples, 1161914 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:02,669 : INFO : EPOCH 9 - PROGRESS: at 56.93% examples, 1165452 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:03,674 : INFO : EPOCH 9 - PROGRESS: at 61.03% examples, 1166429 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:16:04,677 : INFO : EPOCH 9 - PROGRESS: at 65.37% examples, 1169312 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:05,688 : INFO : EPOCH 9 - PROGRESS: at 69.53% examples, 1173711 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:06,690 : INFO : EPOCH 9 - PROGRESS: at 72.60% examples, 1160956 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:07,698 : INFO : EPOCH 9 - PROGRESS: at 75.61% examples, 1146845 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:08,714 : INFO : EPOCH 9 - PROGRESS: at 78.57% examples, 1136304 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:16:09,740 : INFO : EPOCH 9 - PROGRESS: at 82.02% examples, 1131148 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:16:10,776 : INFO : EPOCH 9 - PROGRESS: at 84.91% examples, 1119413 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:11,776 : INFO : EPOCH 9 - PROGRESS: at 88.06% examples, 1110041 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:12,793 : INFO : EPOCH 9 - PROGRESS: at 91.30% examples, 1101748 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:13,793 : INFO : EPOCH 9 - PROGRESS: at 94.26% examples, 1092753 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:14,819 : INFO : EPOCH 9 - PROGRESS: at 97.26% examples, 1083649 words/s, in_qsize 17, out_qsize 6\n",
      "2019-03-15 17:16:15,561 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:16:15,577 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:16:15,582 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:16:15,586 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:16:15,594 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:16:15,614 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:16:15,624 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:16:15,629 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:16:15,637 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:16:15,648 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:16:15,650 : INFO : EPOCH - 9 : training on 41519358 raw words (30350769 effective words) took 28.1s, 1079729 effective words/s\n",
      "2019-03-15 17:16:16,668 : INFO : EPOCH 10 - PROGRESS: at 2.71% examples, 841689 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:17,671 : INFO : EPOCH 10 - PROGRESS: at 5.84% examples, 894722 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:18,676 : INFO : EPOCH 10 - PROGRESS: at 8.72% examples, 894791 words/s, in_qsize 16, out_qsize 3\n",
      "2019-03-15 17:16:19,698 : INFO : EPOCH 10 - PROGRESS: at 11.08% examples, 885470 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:20,704 : INFO : EPOCH 10 - PROGRESS: at 13.60% examples, 885769 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:21,716 : INFO : EPOCH 10 - PROGRESS: at 17.12% examples, 945570 words/s, in_qsize 19, out_qsize 2\n",
      "2019-03-15 17:16:22,716 : INFO : EPOCH 10 - PROGRESS: at 20.49% examples, 991819 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:23,731 : INFO : EPOCH 10 - PROGRESS: at 24.05% examples, 1024447 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:24,731 : INFO : EPOCH 10 - PROGRESS: at 28.65% examples, 1055461 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:25,733 : INFO : EPOCH 10 - PROGRESS: at 33.13% examples, 1073584 words/s, in_qsize 18, out_qsize 1\n",
      "2019-03-15 17:16:26,734 : INFO : EPOCH 10 - PROGRESS: at 37.55% examples, 1093274 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:27,746 : INFO : EPOCH 10 - PROGRESS: at 42.32% examples, 1110060 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:28,761 : INFO : EPOCH 10 - PROGRESS: at 46.76% examples, 1118715 words/s, in_qsize 19, out_qsize 6\n",
      "2019-03-15 17:16:29,762 : INFO : EPOCH 10 - PROGRESS: at 51.34% examples, 1133337 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:30,779 : INFO : EPOCH 10 - PROGRESS: at 55.24% examples, 1133895 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:16:31,782 : INFO : EPOCH 10 - PROGRESS: at 58.27% examples, 1117983 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:32,803 : INFO : EPOCH 10 - PROGRESS: at 61.24% examples, 1101233 words/s, in_qsize 20, out_qsize 1\n",
      "2019-03-15 17:16:33,823 : INFO : EPOCH 10 - PROGRESS: at 64.56% examples, 1089193 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:34,833 : INFO : EPOCH 10 - PROGRESS: at 67.47% examples, 1077767 words/s, in_qsize 20, out_qsize 2\n",
      "2019-03-15 17:16:35,841 : INFO : EPOCH 10 - PROGRESS: at 70.63% examples, 1072069 words/s, in_qsize 20, out_qsize 0\n",
      "2019-03-15 17:16:36,847 : INFO : EPOCH 10 - PROGRESS: at 73.86% examples, 1065068 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:37,870 : INFO : EPOCH 10 - PROGRESS: at 76.66% examples, 1056215 words/s, in_qsize 19, out_qsize 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:16:38,893 : INFO : EPOCH 10 - PROGRESS: at 79.72% examples, 1050633 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:39,899 : INFO : EPOCH 10 - PROGRESS: at 82.75% examples, 1044759 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:40,922 : INFO : EPOCH 10 - PROGRESS: at 85.94% examples, 1041698 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:41,936 : INFO : EPOCH 10 - PROGRESS: at 91.08% examples, 1056164 words/s, in_qsize 19, out_qsize 0\n",
      "2019-03-15 17:16:42,941 : INFO : EPOCH 10 - PROGRESS: at 95.85% examples, 1068663 words/s, in_qsize 17, out_qsize 2\n",
      "2019-03-15 17:16:43,708 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2019-03-15 17:16:43,710 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2019-03-15 17:16:43,712 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2019-03-15 17:16:43,723 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2019-03-15 17:16:43,728 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2019-03-15 17:16:43,733 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-03-15 17:16:43,736 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-15 17:16:43,739 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-15 17:16:43,742 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-15 17:16:43,743 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-15 17:16:43,743 : INFO : EPOCH - 10 : training on 41519358 raw words (30349799 effective words) took 28.1s, 1080572 effective words/s\n",
      "2019-03-15 17:16:43,745 : INFO : training on a 415193580 raw words (303491888 effective words) took 243.0s, 1248865 effective words/s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(303491888, 415193580)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "model.train(documents,total_examples=len(documents),epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-15 17:23:04,440 : INFO : saving Word2Vec object under word2vec.model, separately None\n",
      "2019-03-15 17:23:04,443 : INFO : storing np array 'vectors' to word2vec.model.wv.vectors.npy\n",
      "2019-03-15 17:23:04,481 : INFO : not storing attribute vectors_norm\n",
      "2019-03-15 17:23:04,481 : INFO : storing np array 'syn1neg' to word2vec.model.trainables.syn1neg.npy\n",
      "2019-03-15 17:23:04,507 : INFO : not storing attribute cum_table\n",
      "2019-03-15 17:23:04,634 : INFO : saved word2vec.model\n"
     ]
    }
   ],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nagra-p/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-9.84803140e-01,  2.00643569e-01,  5.01873302e+00,  1.17632627e+00,\n",
       "       -5.69037721e-02, -2.98790812e-01, -3.36786842e+00,  1.83894411e-01,\n",
       "       -8.36263120e-01, -1.74371469e+00,  2.84413385e+00, -5.46091139e-01,\n",
       "        1.71670055e+00,  5.68743324e+00, -1.21971440e+00,  4.10626745e+00,\n",
       "       -2.16679168e+00,  2.62755990e+00, -3.21339607e+00, -3.73317885e+00,\n",
       "        2.33368683e+00,  1.34934640e+00,  1.18614173e+00, -1.35659873e+00,\n",
       "        1.60746694e+00, -2.86103654e+00, -4.39756584e+00, -9.01412293e-02,\n",
       "       -1.48805439e+00, -2.22964835e+00, -2.99705911e+00,  2.48890936e-01,\n",
       "       -2.63185835e+00,  1.43324566e+00, -2.87168241e+00,  4.55185795e+00,\n",
       "       -1.13603115e+00,  2.94469404e+00, -6.78085625e-01, -7.97326803e-01,\n",
       "        1.49115717e+00, -4.09721470e+00,  3.45179248e+00,  2.17270970e+00,\n",
       "       -2.68589115e+00,  4.11349630e+00,  2.97663569e-01, -1.53284514e+00,\n",
       "       -2.81973720e+00, -6.22105122e-01,  4.22532511e+00,  6.00296688e+00,\n",
       "       -5.69216108e+00,  5.88265240e-01,  5.84504700e+00,  3.40172482e+00,\n",
       "        2.75143385e+00, -1.69415474e+00,  2.89656162e+00, -1.00082481e+00,\n",
       "       -2.62207855e-02, -7.53231335e+00,  3.87823224e+00, -1.53360438e+00,\n",
       "        1.84830058e+00, -1.98431575e+00, -1.22557783e+00,  8.01665962e-01,\n",
       "        4.51902986e-01, -2.78947210e+00, -3.00835061e+00,  4.27112770e+00,\n",
       "        3.00833106e+00, -1.70201111e+00,  8.85638833e-01, -2.10264826e+00,\n",
       "        9.06515718e-01,  3.42390347e+00,  3.37382460e+00, -2.46934518e-01,\n",
       "        2.89997101e+00,  1.58045828e+00, -2.23653173e+00,  1.92203093e+00,\n",
       "        1.05512512e+00, -1.48972943e-02, -2.82738339e-02, -4.81154650e-01,\n",
       "        1.66311359e+00,  8.76282826e-02, -2.28931785e+00,  1.00629210e+00,\n",
       "       -2.75005269e+00, -3.63490486e+00, -1.10732067e+00,  3.95967937e+00,\n",
       "        3.23992109e+00,  2.53490031e-01,  1.58701491e+00, -2.72478986e+00,\n",
       "       -1.33726573e+00,  9.90084633e-02, -2.89495015e+00,  2.23335242e+00,\n",
       "        4.58653307e+00, -3.08902478e+00, -2.57985210e+00, -1.21885920e+00,\n",
       "        2.01234055e+00,  3.15799069e+00,  8.90980300e-04, -9.11213517e-01,\n",
       "        1.15458414e-01, -1.67981935e+00,  5.24645150e-01, -6.55489326e-01,\n",
       "       -1.45952487e+00,  5.45346260e-01,  2.06361032e+00, -8.62058938e-01,\n",
       "        4.70964849e-01,  2.32704163e+00, -1.65576303e+00,  2.57137477e-01,\n",
       "        2.81747890e+00,  2.68189937e-01, -2.72634149e-01,  1.23485291e+00,\n",
       "       -2.25496316e+00, -1.29994631e+00, -5.46851754e-01,  9.87280190e-01,\n",
       "       -1.24328542e+00,  1.76699305e+00,  3.64139318e-01,  6.68077290e-01,\n",
       "       -2.84513664e+00, -2.00619102e+00, -2.44639111e+00,  1.44420767e+00,\n",
       "        8.72751474e-01, -4.87428570e+00,  2.61264622e-01, -3.39619589e+00,\n",
       "        1.15404153e+00,  8.28740954e-01,  8.08356047e-01,  2.90360808e-01,\n",
       "        8.11645389e-01,  7.26187181e+00], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['good']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's look at some output \n",
    "This first example shows a simple case of looking up words similar to the word `dirty`. All we need to do here is to call the `most_similar` function and provide the word `dirty` as the positive example. This returns the top 10 similar words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fell', 0.6529394388198853),\n",
       " ('falling', 0.5994173288345337),\n",
       " ('fallen', 0.5769830942153931),\n",
       " ('kick', 0.5465656518936157),\n",
       " ('bump', 0.527692973613739),\n",
       " ('wind', 0.5063945055007935),\n",
       " ('jump', 0.5036382675170898),\n",
       " ('lie', 0.49928027391433716),\n",
       " ('survive', 0.4979685842990875),\n",
       " ('cut', 0.4975726902484894)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "w1 = \"fall\"\n",
    "model.wv.most_similar(positive=w1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks pretty good, right? Let's look at a few more. Let's look at similarity for `polite`, `france` and `shocked`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('courteous', 0.9269418716430664),\n",
       " ('friendly', 0.824870765209198),\n",
       " ('professional', 0.8123276233673096),\n",
       " ('cordial', 0.7933465242385864),\n",
       " ('attentive', 0.7925757765769958),\n",
       " ('personable', 0.7628620862960815)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'polite'\n",
    "w1 = [\"polite\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('latte', 0.6595109701156616),\n",
       " ('pastry', 0.6219616532325745),\n",
       " ('muffin', 0.6001742482185364),\n",
       " ('cappucino', 0.5951939821243286),\n",
       " ('cappuccino', 0.5943077802658081),\n",
       " ('bagel', 0.5895130038261414)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'france'\n",
    "w1 = [\"java\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('horrified', 0.80775386095047),\n",
       " ('amazed', 0.7797470092773438),\n",
       " ('astonished', 0.7748459577560425),\n",
       " ('dismayed', 0.7680633068084717),\n",
       " ('stunned', 0.7603034973144531),\n",
       " ('appalled', 0.7466776371002197)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up top 6 words similar to 'shocked'\n",
    "w1 = [\"shocked\"]\n",
    "model.wv.most_similar (positive=w1,topn=6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's, nice. You can even specify several positive examples to get things that are related in the provided context and provide negative examples to say what should not be considered as related. In the example below we are asking for all items that *relate to bed* only:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('duvet', 0.7086508274078369),\n",
       " ('blanket', 0.7016597390174866),\n",
       " ('mattress', 0.7002605199813843),\n",
       " ('quilt', 0.6868821978569031),\n",
       " ('matress', 0.6777950525283813),\n",
       " ('pillowcase', 0.6413239240646362),\n",
       " ('sheets', 0.6382123827934265),\n",
       " ('foam', 0.6322235465049744),\n",
       " ('pillows', 0.6320573687553406),\n",
       " ('comforter', 0.5972476601600647)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get everything related to stuff on the bed\n",
    "w1 = [\"bed\",'sheet','pillow']\n",
    "w2 = ['couch']\n",
    "model.wv.most_similar (positive=w1,negative=w2,topn=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity between two words in the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can even use the Word2Vec model to return the similarity between two words that are present in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76181122646029453"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two different words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"smelly\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two identical words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"dirty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2682045473522595"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# similarity between two unrelated words\n",
    "model.wv.similarity(w1=\"dirty\",w2=\"clean\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, the above three snippets computes the cosine similarity between the two specified words using word vectors of each. From the scores, it makes sense that `dirty` is highly similar to `smelly` but `dirty` is dissimilar to `clean`. If you do a similarity between two identical words, the score will be 1.0 as the range of the cosine similarity score will always be between [0.0-1.0]. You can read more about cosine similarity scoring [here](https://en.wikipedia.org/wiki/Cosine_similarity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the odd one out\n",
    "You can even use Word2Vec to find odd items given a list of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'france'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"cat\",\"dog\",\"france\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'shower'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Which one is the odd one out in this list?\n",
    "model.wv.doesnt_match([\"bed\",\"pillow\",\"duvet\",\"shower\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding some of the parameters\n",
    "To train the model earlier, we had to set some parameters. Now, let's try to understand what some of them mean. For reference, this is the command that we used to train the model.\n",
    "\n",
    "```\n",
    "model = gensim.models.Word2Vec (documents, size=150, window=10, min_count=2, workers=10)\n",
    "```\n",
    "\n",
    "### `size`\n",
    "The size of the dense vector to represent each token or word. If you have very limited data, then size should be a much smaller value. If you have lots of data, its good to experiment with various sizes. A value of 100-150 has worked well for me. \n",
    "\n",
    "### `window`\n",
    "The maximum distance between the target word and its neighboring word. If your neighbor's position is greater than the maximum window width to the left and the right, then, some neighbors are not considered as being related to the target word. In theory, a smaller window should give you terms that are more related. If you have lots of data, then the window size should not matter too much, as long as its a decent sized window. \n",
    "\n",
    "### `min_count`\n",
    "Minimium frequency count of words. The model would ignore words that do not statisfy the `min_count`. Extremely infrequent words are usually unimportant, so its best to get rid of those. Unless your dataset is really tiny, this does not really affect the model.\n",
    "\n",
    "### `workers`\n",
    "How many threads to use behind the scenes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When should you use Word2Vec?\n",
    "\n",
    "There are many application scenarios for Word2Vec. Imagine if you need to build a sentiment lexicon. Training a Word2Vec model on large amounts of user reviews helps you achieve that. You have a lexicon for not just sentiment, but for most words in the vocabulary. \n",
    "\n",
    "Beyond, raw unstructured text data, you could also use Word2Vec for more structured data. For example, if you had tags for a million stackoverflow questions and answers, you could find tags that are related to a given tag and recommend the related ones for exploration. You can do this by treating each set of co-occuring tags as a \"sentence\" and train a Word2Vec model on this data. Granted, you still need a large number of examples to make it work. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
